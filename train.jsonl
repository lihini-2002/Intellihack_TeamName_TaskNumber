{"conversations": [{"role": "user", "content": "What are the characteristics of reinforcement learning as described in the paper?"}, {"role": "assistant", "content": "Reasoning-oriented Reinforcement Learning\nAfter fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale"}]}
{"conversations": [{"role": "user", "content": "How can Responses be implemented?"}, {"role": "assistant", "content": "Responses may mix multiple languages or lack markdown formatting to\nhighlight answers for users."}]}
{"conversations": [{"role": "user", "content": "How does the paper define reinforcement learning?"}, {"role": "assistant", "content": "Reinforcement Learning for all Scenarios\nTo further align the model with human preferences, we implement a secondary reinforcement\nlearning stage aimed at improving the model\u2019s helpfulness and harmlessness while simultane-\nously refining its reasoning capabilities."}]}
{"conversations": [{"role": "user", "content": "What are the key components of Ruisong Zhang\nRuizhe Pan\nRunji Wang\nR?"}, {"role": "assistant", "content": "Mingchuan Zhang\nMinghua Zhang\nMinghui Tang\nMingxu Zhou\nMeng Li\nMiaojun Wang\nMingming Li\nNing Tian\nPanpan Huang\nPeng Zhang\nQiancheng Wang\nQinyu Chen\nQiushi Du\nRuiqi Ge*\nRuisong Zhang\nRuizhe Pan\nRunji Wang\nR.J."}]}
{"conversations": [{"role": "user", "content": "What are the limitations of SimpleQA?"}, {"role": "assistant", "content": "Codeforces (Rating) 717 759 1134 1820 2061 2029\nSWE Verified (Resolved) 50.8 38.8 42.0 41.6 48.9 49.2\nAider-Polyglot (Acc.) 45.3 16.0 49.6 32.9 61.7 53.3\nMath\nAIME 2024 (Pass@1) 16.0 9.3 39.2 63.6 79.2 79.8\nMATH-500 (Pass@1) 78.3 74.6 90.2 90.0 96.4 97.3\nCNMO 2024 (Pass@1) 13.1 10.8 43.2 67.6 - 78.8\nChinese\nCLUEWSC (EM) 85.4 87.9 90.9 89.9 - 92.8\nC-Eval (EM) 76.7 76.0 86.5 68.9 - 91.8\nC-SimpleQA (Correct) 55.4 58.7 68.0 40.3 - 63.7\nTable 4 |Comparison between DeepSeek-R1 and other representative models."}]}
{"conversations": [{"role": "user", "content": "What is fine-tuning according to the paper?"}, {"role": "assistant", "content": "Rejection Sampling and Supervised Fine-Tuning\nWhen reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT\n(Supervised Fine-Tuning) data for the subsequent round."}]}
{"conversations": [{"role": "user", "content": "What challenges are associated with reinforcement learning according to the research?"}, {"role": "assistant", "content": "Reinforcement Learning\nIn Section 3.2, we can see that by distilling DeepSeek-R1, the small model can achieve impressive\nresults."}]}
{"conversations": [{"role": "user", "content": "What are the limitations of Zhu\nYanping Huang\nYaohui Li\nYi Zheng\nYuchen Zhu\nYunxian Ma\nYing Tang\nYukun Zha\nYuting Yan\nZ?"}, {"role": "assistant", "content": "Zhu\nYanping Huang\nYaohui Li\nYi Zheng\nYuchen Zhu\nYunxian Ma\nYing Tang\nYukun Zha\nYuting Yan\nZ.Z."}]}
{"conversations": [{"role": "user", "content": "Explain the concept of Table."}, {"role": "assistant", "content": "The\nexperimental results, shown in Table 6, demonstrate that the 32B base model, after large-scale\n14"}]}
{"conversations": [{"role": "user", "content": "What are the key components of Introducing SimpleQA?"}, {"role": "assistant", "content": "Introducing SimpleQA, 2024c."}]}
{"conversations": [{"role": "user", "content": "What are the key components of GPT mentioned in the research?"}, {"role": "assistant", "content": "The results are promising: DeepSeek-R1-Distill-Qwen-1.5B outperforms GPT-4o\nand Claude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH."}]}
{"conversations": [{"role": "user", "content": "What are the characteristics of reinforcement learning as described in the paper?"}, {"role": "assistant", "content": "reinforcement learning process, and retraining the reward model needs additional training\nresources and it complicates the whole training pipeline."}]}
{"conversations": [{"role": "user", "content": "What is the relationship between Behaviors and This?"}, {"role": "assistant", "content": "Behaviors such as reflection\u2014where the model\nrevisits and reevaluates its previous steps\u2014and the exploration of alternative approaches to"}]}
{"conversations": [{"role": "user", "content": "Compare Chain and OpenAI."}, {"role": "assistant", "content": "In the context of reasoning capabilities, OpenAI\u2019s o1 (OpenAI, 2024b) series models\nwere the first to introduce inference-time scaling by increasing the length of the Chain-of-\nThought reasoning process."}]}
{"conversations": [{"role": "user", "content": "What is LLMs?"}, {"role": "assistant", "content": "In addition to standard benchmarks, we\nalso evaluate our models on open-ended generation tasks using LLMs as judges."}]}
{"conversations": [{"role": "user", "content": "Explain the concept of LiveCodeBench."}, {"role": "assistant", "content": "2024d), Aider 1, LiveCodeBench (Jain et al., 2024) (2024-08 \u2013 2025-01), Codeforces 2, Chinese\nNational High School Mathematics Olympiad (CNMO 2024)3, and American Invitational Math-\nematics Examination 2024 (AIME 2024) (MAA, 2024). For distilled models, we\nreport representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and\nLiveCodeBench."}]}
{"conversations": [{"role": "user", "content": "What are the advantages of Training?"}, {"role": "assistant", "content": "Contributions\nPost-Training: Large-Scale Reinforcement Learning on the Base Model\n\u2022 We directly apply RL to the base model without relying on supervised fine-tuning (SFT) as\na preliminary step."}]}
{"conversations": [{"role": "user", "content": "What is the relationship between OpenAI and Artificial General Intelligence?"}, {"role": "assistant", "content": "Introduction\nIn recent years, Large Language Models (LLMs) have been undergoing rapid iteration and\nevolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap\ntowards Artificial General Intelligence (AGI). In the context of reasoning capabilities, OpenAI\u2019s o1 (OpenAI, 2024b) series models\nwere the first to introduce inference-time scaling by increasing the length of the Chain-of-\nThought reasoning process."}]}
{"conversations": [{"role": "user", "content": "How can Balaji be implemented?"}, {"role": "assistant", "content": "Balaji, S."}]}
{"conversations": [{"role": "user", "content": "What are the advantages of pre-training over LLM mentioned in the paper?"}, {"role": "assistant", "content": "Introduction\nIn recent years, Large Language Models (LLMs) have been undergoing rapid iteration and\nevolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap\ntowards Artificial General Intelligence (AGI). It has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt\nto user preferences, all while requiring relatively minimal computational resources against\npre-training."}]}
{"conversations": [{"role": "user", "content": "What are the characteristics of reinforcement learning as described in the paper?"}, {"role": "assistant", "content": "In this section, we\nexplore the potential of LLMs to develop reasoning capabilities without any supervised data,\nfocusing on their self-evolution through a pure reinforcement learning process. Reinforcement Learning Algorithm\nGroup Relative Policy OptimizationIn order to save the training costs of RL, we adopt Group\nRelative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is\ntypically the same size as the policy model, and estimates the baseline from group scores instead."}]}
{"conversations": [{"role": "user", "content": "How does the paper define reinforcement learning?"}, {"role": "assistant", "content": "Mastering chess and\nshogi by self-play with a general reinforcement learning algorithm."}]}
{"conversations": [{"role": "user", "content": "How can DeepSeek be implemented?"}, {"role": "assistant", "content": "As depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improve-\n7"}]}
{"conversations": [{"role": "user", "content": "What is distillation according to the paper?"}, {"role": "assistant", "content": "However, there is still one question left: can the model achieve comparable performance\nthrough the large-scale RL training discussed in the paper without distillation?"}]}
{"conversations": [{"role": "user", "content": "How does the paper define language model?"}, {"role": "assistant", "content": "Self-consistency improves chain of thought reasoning in language models."}]}
{"conversations": [{"role": "user", "content": "Compare Resolved and Pass."}, {"role": "assistant", "content": "AIME 2024\n(Pass@1)\nCodeforces\n(Percentile)\nGPQA Diamond\n(Pass@1)\nMATH-500\n(Pass@1)\nMMLU\n(Pass@1)\nSWE-bench Verified\n(Resolved)\n0\n20\n40\n60\n80\n100Accuracy / Percentile (%)\n79.8\n96.3\n71.5\n97.3\n90.8\n49.2\n79.2\n96.6\n75.7\n96.4\n91.8\n48.9\n72.6\n90.6\n62.1\n94.3\n87.4\n36.8\n63.6\n93.4\n60.0\n90.0\n85.2\n41.6\n39.2\n58.7 59.1\n90.2\n88.5\n42.0\nDeepSeek-R1 OpenAI-o1-1217 DeepSeek-R1-32B OpenAI-o1-mini DeepSeek-V3\nFigure 1 |Benchmark performance of DeepSeek-R1."}]}
{"conversations": [{"role": "user", "content": "What is DeepSeek?"}, {"role": "assistant", "content": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\nReinforcement Learning\nDeepSeek-AI\nresearch@deepseek.com\nAbstract\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without super-\nvised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing\nreasoning behaviors."}]}
{"conversations": [{"role": "user", "content": "What are the key components of Compared?"}, {"role": "assistant", "content": "Compared to DeepSeek-R1-Zero, the advantages of cold start data\n9"}]}
{"conversations": [{"role": "user", "content": "What is inference according to the paper?"}, {"role": "assistant", "content": "In the context of reasoning capabilities, OpenAI\u2019s o1 (OpenAI, 2024b) series models\nwere the first to introduce inference-time scaling by increasing the length of the Chain-of-\nThought reasoning process."}]}
{"conversations": [{"role": "user", "content": "Explain the concept of Prompting Engineering."}, {"role": "assistant", "content": "\u2022 Prompting Engineering:When evaluating DeepSeek-R1, we observe that it is sensitive\nto prompts."}]}
{"conversations": [{"role": "user", "content": "What is the purpose of RAG in the context of this research?"}, {"role": "assistant", "content": "Figure 3 |The average response length of DeepSeek-R1-Zero on the training set during the RL\nprocess. DeepSeek-R1-Zero naturally acquires the\nability to solve increasingly complex reasoning tasks by leveraging extended test-time compu-\ntation."}]}
{"conversations": [{"role": "user", "content": "Explain the concept of Supervised Fine."}, {"role": "assistant", "content": "10\n2.3.3 Rejection Sampling and Supervised Fine-Tuning ."}]}
{"conversations": [{"role": "user", "content": "How does the paper define benchmarks?"}, {"role": "assistant", "content": "From\ncrowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline."}]}
{"conversations": [{"role": "user", "content": "How can Specifically be implemented?"}, {"role": "assistant", "content": "Specifically, we train the model using a combination"}]}
{"conversations": [{"role": "user", "content": "What dataset was used to test language model in this research?"}, {"role": "assistant", "content": "In this paper, we take the first step toward improving language model reasoning capabilities\nusing pure reinforcement learning (RL)."}]}
{"conversations": [{"role": "user", "content": "What are the advantages of Huang?"}, {"role": "assistant", "content": "Huang, Y."}]}
{"conversations": [{"role": "user", "content": "How does Zero work?"}, {"role": "assistant", "content": "DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start\ndata, achieving strong performance across various tasks."}]}
{"conversations": [{"role": "user", "content": "What is the purpose of DeepSeek?"}, {"role": "assistant", "content": "On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,\ndemonstrating its capability in handling fact-based queries."}]}
{"conversations": [{"role": "user", "content": "What are the limitations of Mingchuan Zhang\nMinghua Zhang\nMinghui Tang\nMingxu Zhou\nMeng Li\nMiaojun Wang\nMingming Li\nNing Tian\nPanpan Huang\nPeng Zhang\nQiancheng Wang\nQinyu Chen\nQiushi Du\nRuiqi Ge?"}, {"role": "assistant", "content": "Mingchuan Zhang\nMinghua Zhang\nMinghui Tang\nMingxu Zhou\nMeng Li\nMiaojun Wang\nMingming Li\nNing Tian\nPanpan Huang\nPeng Zhang\nQiancheng Wang\nQinyu Chen\nQiushi Du\nRuiqi Ge*\nRuisong Zhang\nRuizhe Pan\nRunji Wang\nR.J."}]}
{"conversations": [{"role": "user", "content": "What are the limitations of Chiang?"}, {"role": "assistant", "content": "Chiang, E."}]}
{"conversations": [{"role": "user", "content": "What is Qwen2?"}, {"role": "assistant", "content": "Qwen2.5: A party of foundation models, 2024b."}]}
{"conversations": [{"role": "user", "content": "What is the purpose of This?"}, {"role": "assistant", "content": "This behavior is not only a testament to the model\u2019s growing reasoning abilities\nbut also a captivating example of how reinforcement learning can lead to unexpected and\nsophisticated outcomes. This moment is not only an \u201caha moment\u201d for the model but also for the researchers\nobserving its behavior."}]}
{"conversations": [{"role": "user", "content": "What are the characteristics of language model as described in the paper?"}, {"role": "assistant", "content": "Livecodebench: Holistic and contamination free evaluation of large language models for code."}]}
{"conversations": [{"role": "user", "content": "How does Baker work?"}, {"role": "assistant", "content": "Baker, T."}]}
{"conversations": [{"role": "user", "content": "What are the characteristics of benchmarks as described in the paper?"}, {"role": "assistant", "content": "Evaluation Prompts Following the setup in DeepSeek-V3, standard benchmarks such as\nMMLU, DROP , GPQA Diamond, and SimpleQA are evaluated using prompts from the simple-\nevals framework. For code and math benchmarks, the\nHumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++,\nC#, JavaScript, TypeScript, PHP , and Bash)."}]}
{"conversations": [{"role": "user", "content": "How does the paper define architecture?"}, {"role": "assistant", "content": "DeepSeek-R1 Evaluation\nBenchmark (Metric)\nClaude-3.5- GPT-4o DeepSeek OpenAI OpenAI DeepSeek\nSonnet-1022 0513 V3 o1-mini o1-1217 R1\nArchitecture - - MoE - - MoE\n# Activated Params - - 37B - - 37B\n# Total Params - - 671B - - 671B\nEnglish\nMMLU (Pass@1) 88.3 87.2 88.5 85.2 91.8 90.8\nMMLU-Redux (EM) 88.9 88.0 89.1 86.7 - 92.9\nMMLU-Pro (EM) 78.0 72.6 75.9 80.3 - 84.0\nDROP (3-shot F1) 88.3 83.7 91.6 83.9 90.2 92.2\nIF-Eval (Prompt Strict) 86.5 84.3 86.1 84.8 - 83.3\nGPQA Diamond (Pass@1) 65.0 49.9 59.1 60.0 75.7 71.5\nSimpleQA (Correct) 28.4 38.2 24.9 7.0 47.0 30.1\nFRAMES (Acc.) 72.5 80.5 73.3 76.9 - 82.5\nAlpacaEval2.0 (LC-winrate) 52.0 51.1 70.0 57.8 - 87.6\nArenaHard (GPT-4-1106) 85.2 80.4 85.5 92.0 - 92.3\nCode\nLiveCodeBench (Pass@1-COT) 38.9 32.9 36.2 53.8 63.4 65.9\nCodeforces (Percentile) 20.3 23.6 58.7 93.4 96.6 96.3\nCodeforces (Rating) 717 759 1134 1820 2061 2029\nSWE Verified (Resolved) 50.8 38.8 42.0 41.6 48.9 49.2\nAider-Polyglot (Acc.) 45.3 16.0 49.6 32.9 61.7 53.3\nMath"}]}
{"conversations": [{"role": "user", "content": "What is the purpose of bias in the context of this research?"}, {"role": "assistant", "content": "We intentionally limit our constraints to this structural format, avoiding any content-specific\nbiases\u2014such as mandating reflective reasoning or promoting particular problem-solving strate-\ngies\u2014to ensure that we can accurately observe the model\u2019s natural progression during the RL\nprocess."}]}
{"conversations": [{"role": "user", "content": "Explain the concept of AIME."}, {"role": "assistant", "content": "Model AIME 2024 MATH-500 GPQA LiveCode CodeForcesDiamond Bench\npass@1 cons@64 pass@1 pass@1 pass@1 rating\nOpenAI-o1-mini 63.6 80.0 90.0 60.0 53.8 1820\nOpenAI-o1-0912 74.4 83.3 94.8 77.3 63.4 1843\nDeepSeek-R1-Zero 71.0 86.7 95.9 73.3 50.0 1444\nTable 2 |Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related\nbenchmarks. Figure 2 |AIME accuracy of DeepSeek-R1-Zero during training. For example,\nwhen majority voting is employed on the AIME benchmark, DeepSeek-R1-Zero\u2019s performance"}]}
{"conversations": [{"role": "user", "content": "What are the key components of benchmarks mentioned in the research?"}, {"role": "assistant", "content": "Model AIME 2024 MATH-500 GPQA LiveCode CodeForcesDiamond Bench\npass@1 cons@64 pass@1 pass@1 pass@1 rating\nOpenAI-o1-mini 63.6 80.0 90.0 60.0 53.8 1820\nOpenAI-o1-0912 74.4 83.3 94.8 77.3 63.4 1843\nDeepSeek-R1-Zero 71.0 86.7 95.9 73.3 50.0 1444\nTable 2 |Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related\nbenchmarks."}]}
{"conversations": [{"role": "user", "content": "What is Mancino?"}, {"role": "assistant", "content": "Mancino, R."}]}
{"conversations": [{"role": "user", "content": "How does Contributions work?"}, {"role": "assistant", "content": "Contents\n1 Introduction 3\n1.1 Contributions ."}]}
{"conversations": [{"role": "user", "content": "What is reinforcement learning according to the paper?"}, {"role": "assistant", "content": "Several prior\nworks have explored various approaches, including process-based reward models (Lightman\net al., 2023; Uesato et al., 2022; Wang et al., 2023), reinforcement learning (Kumar et al., 2024),\nand search algorithms such as Monte Carlo Tree Search and Beam Search (Feng et al., 2024; Trinh\net al., 2024; Xin et al., 2024). In this paper, we take the first step toward improving language model reasoning capabilities\nusing pure reinforcement learning (RL)."}]}
{"conversations": [{"role": "user", "content": "What is the purpose of DeepSeek?"}, {"role": "assistant", "content": "instance, DeepSeek-R1 might use English for reasoning and responses, even if the query is\nin a language other than English or Chinese. \u2022 Prompting Engineering:When evaluating DeepSeek-R1, we observe that it is sensitive\nto prompts. As a result, DeepSeek-R1 has not demonstrated a huge improvement\nover DeepSeek-V3 on software engineering benchmarks."}]}
{"conversations": [{"role": "user", "content": "How does Leike work?"}, {"role": "assistant", "content": "Leike, J."}]}
{"conversations": [{"role": "user", "content": "What are the advantages of MCTS?"}, {"role": "assistant", "content": "In conclusion, while MCTS can improve performance during inference when paired with a\npre-trained value model, iteratively boosting model performance through self-search remains a\nsignificant challenge."}]}
{"conversations": [{"role": "user", "content": "What is the purpose of distillation in the context of this research?"}, {"role": "assistant", "content": "Distillation: Empower Small Models with Reasoning Capability\nTo equip more efficient smaller models with reasoning capabilities like DeepSeek-R1, we directly\nfine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using\nthe 800k samples curated with DeepSeek-R1, as detailed in \u00a72.3.3."}]}
{"conversations": [{"role": "user", "content": "What are the key components of fine-tuning mentioned in the research?"}, {"role": "assistant", "content": "improved through large-scale reinforcement learning (RL), even without using supervised\nfine-tuning (SFT) as a cold start."}]}
{"conversations": [{"role": "user", "content": "How can Finally be implemented?"}, {"role": "assistant", "content": "Finally, we combine the accuracy of\nreasoning tasks and the reward for language consistency by directly summing them to form the\nfinal reward."}]}
{"conversations": [{"role": "user", "content": "How can Harnessing be implemented?"}, {"role": "assistant", "content": "Deepseek-prover-v1.5: Harnessing\nproof assistant feedback for reinforcement learning and monte-carlo tree search, 2024."}]}
{"conversations": [{"role": "user", "content": "How does the paper define reinforcement learning?"}, {"role": "assistant", "content": "We do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero,\nbecause we find that the neural reward model may suffer from reward hacking in the large-scale\nreinforcement learning process, and retraining the reward model needs additional training\nresources and it complicates the whole training pipeline."}]}
{"conversations": [{"role": "user", "content": "Explain the concept of Hong."}, {"role": "assistant", "content": "Hong, A."}]}
{"conversations": [{"role": "user", "content": "What are the characteristics of benchmarks as described in the paper?"}, {"role": "assistant", "content": "Experiment\nBenchmarks We evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema"}]}
{"conversations": [{"role": "user", "content": "How does the paper define benchmarks?"}, {"role": "assistant", "content": "After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance\non reasoning benchmarks."}]}
{"conversations": [{"role": "user", "content": "What are the characteristics of distillation as described in the paper?"}, {"role": "assistant", "content": "Therefore, we can draw two conclusions: First, distilling more powerful models into smaller\nones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in\nthis paper require enormous computational power and may not even achieve the performance\nof distillation. Second, while distillation strategies are both economical and effective, advancing"}]}
{"conversations": [{"role": "user", "content": "What is the purpose of BERT in the context of this research?"}, {"role": "assistant", "content": "Herbert-Voss, W."}]}
{"conversations": [{"role": "user", "content": "What is However?"}, {"role": "assistant", "content": "However, in practice, PRM has three main limitations that may hinder its ultimate suc-\ncess."}]}
{"conversations": [{"role": "user", "content": "What are the characteristics of reinforcement learning as described in the paper?"}, {"role": "assistant", "content": "allowing us to witness the power and beauty of reinforcement learning. DeepSeek-R1: Reinforcement Learning with Cold Start\nInspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can\nreasoning performance be further improved or convergence accelerated by incorporating a small\namount of high-quality data as a cold start?"}]}
{"conversations": [{"role": "user", "content": "What are the key components of fine-tuning mentioned in the research?"}, {"role": "assistant", "content": "After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking\ninto account prompts from all scenarios."}]}
{"conversations": [{"role": "user", "content": "What challenges are associated with reinforcement learning according to the research?"}, {"role": "assistant", "content": "2.3 DeepSeek-R1: Reinforcement Learning with Cold Start . 9\n2.3.2 Reasoning-oriented Reinforcement Learning . 10\n2.3.4 Reinforcement Learning for all Scenarios ."}]}
{"conversations": [{"role": "user", "content": "What are the advantages of STEM?"}, {"role": "assistant", "content": "To answer this question, we conduct large-scale RL training on Qwen-32B-Base using math,\ncode, and STEM data, training for over 10K steps, resulting in DeepSeek-R1-Zero-Qwen-32B."}]}
{"conversations": [{"role": "user", "content": "What are the limitations of First?"}, {"role": "assistant", "content": "First,\nunlike chess, where the search space is relatively well-defined, token generation presents an\n15"}]}
{"conversations": [{"role": "user", "content": "What are the characteristics of bias as described in the paper?"}, {"role": "assistant", "content": "For harmlessness, we evaluate the entire\nresponse of the model, including both the reasoning process and the summary, to identify and\nmitigate any potential risks, biases, or harmful content that may arise during the generation\nprocess."}]}
{"conversations": [{"role": "user", "content": "What is Hilton?"}, {"role": "assistant", "content": "Hilton."}]}
{"conversations": [{"role": "user", "content": "What are the advantages of However?"}, {"role": "assistant", "content": "However, there is still one question left: can the model achieve comparable performance\nthrough the large-scale RL training discussed in the paper without distillation?"}]}
{"conversations": [{"role": "user", "content": "What is the purpose of Li\nY?"}, {"role": "assistant", "content": "Li\nY.Q. Wei\nYang Zhang\nYanhong Xu\nYao Li\nYao Zhao\nYaofeng Sun\nYaohui Wang\nYi Yu\nYichao Zhang\nYifan Shi\nYiliang Xiong\nYing He\nYishi Piao\nYisong Wang\nYixuan Tan\nYiyang Ma*\nYiyuan Liu\nYongqiang Guo\nYuan Ou\nYuduan Wang\nYue Gong\nYuheng Zou\nYujia He\nYunfan Xiong\nYuxiang Luo\nYuxiang You\nYuxuan Liu\nYuyang Zhou\nY.X. Zhu\nYanping Huang\nYaohui Li\nYi Zheng\nYuchen Zhu\nYunxian Ma\nYing Tang\nYukun Zha\nYuting Yan\nZ.Z."}]}
{"conversations": [{"role": "user", "content": "What is benchmarks according to the paper?"}, {"role": "assistant", "content": "Notably, our distilled 14B model outperforms state-of-the-art open-source\nQwQ-32B-Preview (Qwen, 2024a) by a large margin, and the distilled 32B and 70B models set a\nnew record on the reasoning benchmarks among dense models."}]}
{"conversations": [{"role": "user", "content": "How does Math work?"}, {"role": "assistant", "content": "DeepSeek-R1 Evaluation\nBenchmark (Metric)\nClaude-3.5- GPT-4o DeepSeek OpenAI OpenAI DeepSeek\nSonnet-1022 0513 V3 o1-mini o1-1217 R1\nArchitecture - - MoE - - MoE\n# Activated Params - - 37B - - 37B\n# Total Params - - 671B - - 671B\nEnglish\nMMLU (Pass@1) 88.3 87.2 88.5 85.2 91.8 90.8\nMMLU-Redux (EM) 88.9 88.0 89.1 86.7 - 92.9\nMMLU-Pro (EM) 78.0 72.6 75.9 80.3 - 84.0\nDROP (3-shot F1) 88.3 83.7 91.6 83.9 90.2 92.2\nIF-Eval (Prompt Strict) 86.5 84.3 86.1 84.8 - 83.3\nGPQA Diamond (Pass@1) 65.0 49.9 59.1 60.0 75.7 71.5\nSimpleQA (Correct) 28.4 38.2 24.9 7.0 47.0 30.1\nFRAMES (Acc.) 72.5 80.5 73.3 76.9 - 82.5\nAlpacaEval2.0 (LC-winrate) 52.0 51.1 70.0 57.8 - 87.6\nArenaHard (GPT-4-1106) 85.2 80.4 85.5 92.0 - 92.3\nCode\nLiveCodeBench (Pass@1-COT) 38.9 32.9 36.2 53.8 63.4 65.9\nCodeforces (Percentile) 20.3 23.6 58.7 93.4 96.6 96.3\nCodeforces (Rating) 717 759 1134 1820 2061 2029\nSWE Verified (Resolved) 50.8 38.8 42.0 41.6 48.9 49.2\nAider-Polyglot (Acc.) 45.3 16.0 49.6 32.9 61.7 53.3\nMath"}]}
{"conversations": [{"role": "user", "content": "Compare DeepSeek and Chinese SimpleQA."}, {"role": "assistant", "content": "Additionally, DeepSeek-R1\nexcels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis\ncapabilities. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,\ndemonstrating its capability in handling fact-based queries. However, DeepSeek-R1 performs worse than\nDeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse\nanswering certain queries after safety RL."}]}
{"conversations": [{"role": "user", "content": "Compare SWE Verified and Claude."}, {"role": "assistant", "content": "On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms DeepSeek-R1\non Aider but achieves comparable performance on SWE Verified."}]}
{"conversations": [{"role": "user", "content": "What is the relationship between distillation and fine-tuning in this context?"}, {"role": "assistant", "content": "DeepSeek-R1 is more powerful,\nleveraging cold-start data alongside iterative RL fine-tuning. We further explore distillation the reasoning capability to small dense models."}]}
{"conversations": [{"role": "user", "content": "How does Base work?"}, {"role": "assistant", "content": "In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as\nthe starting point for RL."}]}
{"conversations": [{"role": "user", "content": "How does Experiment\nBenchmarks We work?"}, {"role": "assistant", "content": "Experiment\nBenchmarks We evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema"}]}
{"conversations": [{"role": "user", "content": "What are the limitations of DeepSeek?"}, {"role": "assistant", "content": "Figure 3 |The average response length of DeepSeek-R1-Zero on the training set during the RL\nprocess. DeepSeek-R1-Zero naturally learns to solve reasoning tasks with more thinking time. DeepSeek-R1-Zero naturally acquires the\nability to solve increasingly complex reasoning tasks by leveraging extended test-time compu-\ntation."}]}
{"conversations": [{"role": "user", "content": "What is the purpose of DeepSeek?"}, {"role": "assistant", "content": "Training Template\nTo train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides\nthe base model to adhere to our specified instructions. As depicted in Table 1, this template\nrequires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero\nPerformance of DeepSeek-R1-Zero Figure 2 depicts the performance trajectory of DeepSeek-\nR1-Zero on the AIME 2024 benchmark throughout the RL training process."}]}
{"conversations": [{"role": "user", "content": "What is BERT according to the paper?"}, {"role": "assistant", "content": "Hubert, J."}]}
{"conversations": [{"role": "user", "content": "What are the limitations of DeepSeek?"}, {"role": "assistant", "content": "Distillation: Empower Small Models with Reasoning Capability\nTo equip more efficient smaller models with reasoning capabilities like DeepSeek-R1, we directly\nfine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using\nthe 800k samples curated with DeepSeek-R1, as detailed in \u00a72.3.3."}]}
{"conversations": [{"role": "user", "content": "Compare Drawback and Zero Although DeepSeek."}, {"role": "assistant", "content": "Drawback of DeepSeek-R1-Zero Although DeepSeek-R1-Zero exhibits strong reasoning"}]}
{"conversations": [{"role": "user", "content": "Compare Llama and Qwen2."}, {"role": "assistant", "content": "fine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using\nthe 800k samples curated with DeepSeek-R1, as detailed in \u00a72.3.3. The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-\n14B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct. We select Llama-3.3 because its\nreasoning capability is slightly better than that of Llama-3.1."}]}
{"conversations": [{"role": "user", "content": "How does Zero work?"}, {"role": "assistant", "content": "Assistant:\nTable 1 |Template for DeepSeek-R1-Zero. To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two\ntypes of rewards:\n\u2022 Accuracy rewards: The accuracy reward model evaluates whether the response is correct."}]}
{"conversations": [{"role": "user", "content": "What is language model according to the paper?"}, {"role": "assistant", "content": "Evaluating large language models trained on code."}]}
{"conversations": [{"role": "user", "content": "What is the purpose of Zhao?"}, {"role": "assistant", "content": "Zhao, H."}]}
{"conversations": [{"role": "user", "content": "How does the paper define reinforcement learning?"}, {"role": "assistant", "content": "This behavior is not only a testament to the model\u2019s growing reasoning abilities\nbut also a captivating example of how reinforcement learning can lead to unexpected and\nsophisticated outcomes. It underscores the power and beauty of reinforcement learning: rather\nthan explicitly teaching the model on how to solve a problem, we simply provide it with the\nright incentives, and it autonomously develops advanced problem-solving strategies."}]}
{"conversations": [{"role": "user", "content": "What are the key components of Mastering?"}, {"role": "assistant", "content": "Mastering the game of go without human knowledge."}]}
{"conversations": [{"role": "user", "content": "Explain the concept of ZeroA."}, {"role": "assistant", "content": "Aha Moment of DeepSeek-R1-ZeroA particularly intriguing phenomenon observed during\nthe training of DeepSeek-R1-Zero is the occurrence of an \u201caha moment\u201d."}]}
{"conversations": [{"role": "user", "content": "What are the characteristics of LLM as described in the paper?"}, {"role": "assistant", "content": "Math-shepherd: A label-\nfree step-by-step verifier for llms in mathematical reasoning. Agentless: Demystifying llm-based software\nengineering agents."}]}
{"conversations": [{"role": "user", "content": "What are the key components of RAG mentioned in the research?"}, {"role": "assistant", "content": "Notably, the average pass@1 score on AIME 2024 shows a significant\nincrease, jumping from an initial 15.6% to an impressive 71.0%, reaching performance levels\ncomparable to OpenAI-o1-0912."}]}
{"conversations": [{"role": "user", "content": "How does the paper define reinforcement learning?"}, {"role": "assistant", "content": "These behaviors are not explicitly programmed but instead\nemerge as a result of the model\u2019s interaction with the reinforcement learning environment."}]}
{"conversations": [{"role": "user", "content": "What is LLM according to the paper?"}, {"role": "assistant", "content": "In addition to standard benchmarks, we\nalso evaluate our models on open-ended generation tasks using LLMs as judges."}]}
{"conversations": [{"role": "user", "content": "What is the relationship between DeepSeek and SimpleQA?"}, {"role": "assistant", "content": "On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,\ndemonstrating its capability in handling fact-based queries."}]}
{"conversations": [{"role": "user", "content": "What is the relationship between Llama and Dubey?"}, {"role": "assistant", "content": "We open-source the distilled Qwen and Llama (Dubey\net al., 2024) series."}]}
{"conversations": [{"role": "user", "content": "What is fine-tuning according to the paper?"}, {"role": "assistant", "content": "These improvements can be linked to the inclusion\nof instruction-following data during the final stages of supervised fine-tuning (SFT) and RL\ntraining."}]}
{"conversations": [{"role": "user", "content": "How does OpenAI work?"}, {"role": "assistant", "content": "Summary of Evaluation Results\n\u2022 Reasoning tasks: (1) DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly\nsurpassing OpenAI-o1-1217. On MATH-500, it attains an impressive score of 97.3%,\nperforming on par with OpenAI-o1-1217 and significantly outperforming other models."}]}
{"conversations": [{"role": "user", "content": "Compare OpenAI and AIME."}, {"role": "assistant", "content": "Notably, the average pass@1 score on AIME 2024 shows a significant\nincrease, jumping from an initial 15.6% to an impressive 71.0%, reaching performance levels\ncomparable to OpenAI-o1-0912. Table 2 provides a comparative analysis between DeepSeek-R1-Zero and OpenAI\u2019s o1-0912\nmodels across a variety of reasoning-related benchmarks."}]}
{"conversations": [{"role": "user", "content": "What are the characteristics of benchmarks as described in the paper?"}, {"role": "assistant", "content": "The evaluation results demonstrate that\nthe distilled smaller dense models perform exceptionally well on benchmarks."}]}
{"conversations": [{"role": "user", "content": "How does the paper define benchmarks?"}, {"role": "assistant", "content": "The results are promising: DeepSeek-R1-Distill-Qwen-1.5B outperforms GPT-4o\nand Claude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH."}]}
{"conversations": [{"role": "user", "content": "Compare DeepSeek and Using."}, {"role": "assistant", "content": "The open source DeepSeek-R1, as well as its API, will benefit\nthe research community to distill better smaller models in the future. \u2022 Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models\nthat are widely used in the research community. DeepSeek-"}]}
{"conversations": [{"role": "user", "content": "What method was used to evaluate language model in the research?"}, {"role": "assistant", "content": "Pushing the limits of mathematical reasoning in open language models."}]}
{"conversations": [{"role": "user", "content": "How does the paper define benchmarks?"}, {"role": "assistant", "content": "\u2022 Knowledge: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-\nR1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores\nof 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its\nperformance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1\nsurpasses other closed-source models, demonstrating its competitive edge in educational\ntasks."}]}
{"conversations": [{"role": "user", "content": "What is reinforcement learning according to the paper?"}, {"role": "assistant", "content": "In conclusion, while PRM demonstrates a good\nability to rerank the top-N responses generated by the model or assist in guided search (Snell\net al., 2024), its advantages are limited compared to the additional computational overhead it\nintroduces during the large-scale reinforcement learning process in our experiments."}]}
{"conversations": [{"role": "user", "content": "What are the limitations of Reasoning?"}, {"role": "assistant", "content": "Model\nAIME 2024 MATH-500 GPQA Diamond LiveCodeBench\npass@1 cons@64 pass@1 pass@1 pass@1\nQwQ-32B-Preview 50.0 60.0 90.6 54.5 41.9\nDeepSeek-R1-Zero-Qwen-32B 47.0 60.0 91.6 55.0 40.2\nDeepSeek-R1-Distill-Qwen-32B 72.6 83.3 94.3 62.1 57.2\nTable 6 |Comparison of distilled and RL Models on Reasoning-Related Benchmarks."}]}
{"conversations": [{"role": "user", "content": "How can Snell be implemented?"}, {"role": "assistant", "content": "In conclusion, while PRM demonstrates a good\nability to rerank the top-N responses generated by the model or assist in guided search (Snell\net al., 2024), its advantages are limited compared to the additional computational overhead it\nintroduces during the large-scale reinforcement learning process in our experiments."}]}
{"conversations": [{"role": "user", "content": "What is the purpose of Thought?"}, {"role": "assistant", "content": "2) How can we train a user-friendly model that\nnot only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong\ngeneral capabilities?"}]}
{"conversations": [{"role": "user", "content": "What are the limitations of Figure?"}, {"role": "assistant", "content": "As depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improve-\n7"}]}
{"conversations": [{"role": "user", "content": "How can Group\nRelative Policy Optimization be implemented?"}, {"role": "assistant", "content": "Reinforcement Learning Algorithm\nGroup Relative Policy OptimizationIn order to save the training costs of RL, we adopt Group\nRelative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is\ntypically the same size as the policy model, and estimates the baseline from group scores instead."}]}
{"conversations": [{"role": "user", "content": "What are the limitations of OpenAI?"}, {"role": "assistant", "content": "For instance, the pass@1 score on AIME 2024 increases from 15.6% to\n71.0%, and with majority voting, the score further improves to 86.7%, matching the performance\nof OpenAI-o1-0912."}]}
{"conversations": [{"role": "user", "content": "What is reinforcement learning according to the paper?"}, {"role": "assistant", "content": "In this study, we demonstrate that reasoning capabilities can be significantly\nimproved through large-scale reinforcement learning (RL), even without using supervised\nfine-tuning (SFT) as a cold start."}]}
{"conversations": [{"role": "user", "content": "What is reinforcement learning according to the paper?"}, {"role": "assistant", "content": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\nReinforcement Learning\nDeepSeek-AI\nresearch@deepseek.com\nAbstract\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without super-\nvised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities."}]}
{"conversations": [{"role": "user", "content": "What are the advantages of Additionally?"}, {"role": "assistant", "content": "Additionally, the performance of DeepSeek-\nR1-Zero can be further augmented through the application of majority voting."}]}
{"conversations": [{"role": "user", "content": "What is the purpose of Base?"}, {"role": "assistant", "content": "Reasoning-oriented Reinforcement Learning\nAfter fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale\nreinforcement learning training process as employed in DeepSeek-R1-Zero."}]}
{"conversations": [{"role": "user", "content": "Compare DeepSeek and This."}, {"role": "assistant", "content": "DeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial\napproach."}]}
{"conversations": [{"role": "user", "content": "What is MMLU?"}, {"role": "assistant", "content": "AIME 2024\n(Pass@1)\nCodeforces\n(Percentile)\nGPQA Diamond\n(Pass@1)\nMATH-500\n(Pass@1)\nMMLU\n(Pass@1)\nSWE-bench Verified\n(Resolved)\n0\n20\n40\n60\n80\n100Accuracy / Percentile (%)\n79.8\n96.3\n71.5\n97.3\n90.8\n49.2\n79.2\n96.6\n75.7\n96.4\n91.8\n48.9\n72.6\n90.6\n62.1\n94.3\n87.4\n36.8\n63.6\n93.4\n60.0\n90.0\n85.2\n41.6\n39.2\n58.7 59.1\n90.2\n88.5\n42.0\nDeepSeek-R1 OpenAI-o1-1217 DeepSeek-R1-32B OpenAI-o1-mini DeepSeek-V3\nFigure 1 |Benchmark performance of DeepSeek-R1."}]}
{"conversations": [{"role": "user", "content": "What are the key components of DeepSeek?"}, {"role": "assistant", "content": "For reasoning data, we adhere to the\nmethodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the\nlearning process in math, code, and logical reasoning domains. We build\nupon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and train-\ning prompts."}]}
{"conversations": [{"role": "user", "content": "What is the relationship between GPT and RAG in this context?"}, {"role": "assistant", "content": "Specifically, we\nadhere to the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li\net al., 2024), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons."}]}
{"conversations": [{"role": "user", "content": "Compare Silver and MCTS."}, {"role": "assistant", "content": "Monte Carlo Tree Search (MCTS)Inspired by AlphaGo (Silver et al., 2017b) and AlphaZero (Sil-\nver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time\ncompute scalability."}]}
{"conversations": [{"role": "user", "content": "What is the purpose of Fact?"}, {"role": "assistant", "content": "Fact, fetch, and reason: A unified evaluation of retrieval-augmented generation."}]}
{"conversations": [{"role": "user", "content": "How can Overview\nPrevious be implemented?"}, {"role": "assistant", "content": "Overview\nPrevious work has heavily relied on large amounts of supervised data to enhance model\nperformance."}]}
{"conversations": [{"role": "user", "content": "What are the key components of Base?"}, {"role": "assistant", "content": "We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about\n800k samples."}]}
{"conversations": [{"role": "user", "content": "What are the key components of LLM mentioned in the research?"}, {"role": "assistant", "content": "Scaling llm test-time compute optimally can be more\neffective than scaling model parameters, 2024."}]}
{"conversations": [{"role": "user", "content": "What are the key components of fine-tuning mentioned in the research?"}, {"role": "assistant", "content": "Reasoning-oriented Reinforcement Learning\nAfter fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale"}]}
{"conversations": [{"role": "user", "content": "How does the paper define language model?"}, {"role": "assistant", "content": "Chi-\nnese simpleqa: A chinese factuality evaluation for large language models."}]}
{"conversations": [{"role": "user", "content": "Explain the concept of These."}, {"role": "assistant", "content": "These improvements can be linked to the inclusion\nof instruction-following data during the final stages of supervised fine-tuning (SFT) and RL\ntraining."}]}
{"conversations": [{"role": "user", "content": "What are the key components of fine-tuning mentioned in the research?"}, {"role": "assistant", "content": "DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without super-\nvised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities."}]}
{"conversations": [{"role": "user", "content": "Explain the concept of Lillicrap."}, {"role": "assistant", "content": "Lillicrap, K."}]}
{"conversations": [{"role": "user", "content": "What is the purpose of Shao?"}, {"role": "assistant", "content": "Shao, R. Shao, W."}]}
{"conversations": [{"role": "user", "content": "What is the purpose of GPQA Diamond LiveCodeBench?"}, {"role": "assistant", "content": "Model\nAIME 2024 MATH-500 GPQA Diamond LiveCodeBench\npass@1 cons@64 pass@1 pass@1 pass@1\nQwQ-32B-Preview 50.0 60.0 90.6 54.5 41.9\nDeepSeek-R1-Zero-Qwen-32B 47.0 60.0 91.6 55.0 40.2\nDeepSeek-R1-Distill-Qwen-32B 72.6 83.3 94.3 62.1 57.2\nTable 6 |Comparison of distilled and RL Models on Reasoning-Related Benchmarks."}]}
{"conversations": [{"role": "user", "content": "What is benchmarks according to the paper?"}, {"role": "assistant", "content": "crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline."}]}
{"conversations": [{"role": "user", "content": "What is benchmarks according to the paper?"}, {"role": "assistant", "content": "A similar trend is observed on coding algorithm\ntasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these\nbenchmarks."}]}
{"conversations": [{"role": "user", "content": "How does Yiyuan Liu\nYongqiang Guo\nYuan Ou\nYuduan Wang\nYue Gong\nYuheng Zou\nYujia He\nYunfan Xiong\nYuxiang Luo\nYuxiang You\nYuxuan Liu\nYuyang Zhou\nY work?"}, {"role": "assistant", "content": "Wei\nYang Zhang\nYanhong Xu\nYao Li\nYao Zhao\nYaofeng Sun\nYaohui Wang\nYi Yu\nYichao Zhang\nYifan Shi\nYiliang Xiong\nYing He\nYishi Piao\nYisong Wang\nYixuan Tan\nYiyang Ma*\nYiyuan Liu\nYongqiang Guo\nYuan Ou\nYuduan Wang\nYue Gong\nYuheng Zou\nYujia He\nYunfan Xiong\nYuxiang Luo\nYuxiang You\nYuxuan Liu\nYuyang Zhou\nY.X."}]}
{"conversations": [{"role": "user", "content": "How does Unsuccessful Attempts work?"}, {"role": "assistant", "content": "14\n4.2 Unsuccessful Attempts ."}]}
{"conversations": [{"role": "user", "content": "What are the characteristics of inference as described in the paper?"}, {"role": "assistant", "content": "In conclusion, while MCTS can improve performance during inference when paired with a\npre-trained value model, iteratively boosting model performance through self-search remains a\nsignificant challenge."}]}
{"conversations": [{"role": "user", "content": "How does OpenAI work?"}, {"role": "assistant", "content": "After these steps, we obtained a checkpoint referred to\nas DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217."}]}
{"conversations": [{"role": "user", "content": "Compare However and DeepSeek."}, {"role": "assistant", "content": "However, in this stage,\nwe expand the dataset by incorporating additional data, some of which use a generative reward\nmodel by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment."}]}
{"conversations": [{"role": "user", "content": "Compare DeepSeek and These."}, {"role": "assistant", "content": "Additionally, DeepSeek-R1\nexcels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis\ncapabilities. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,\ndemonstrating its capability in handling fact-based queries. However, DeepSeek-R1 performs worse than\nDeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse\nanswering certain queries after safety RL."}]}
{"conversations": [{"role": "user", "content": "What is the purpose of LLM in the context of this research?"}, {"role": "assistant", "content": "Learning to reason with llms, 2024b. URL https://openai.com/index/learnin\ng-to-reason-with-llms/ ."}]}
{"conversations": [{"role": "user", "content": "What are the characteristics of reinforcement learning as described in the paper?"}, {"role": "assistant", "content": "This im-\nprovement is primarily attributed to enhanced accuracy in STEM-related questions, where signif-\nicant gains are achieved through large-scale reinforcement learning."}]}
{"conversations": [{"role": "user", "content": "How does For MMLU work?"}, {"role": "assistant", "content": "For MMLU-Redux, we adopt the Zero-Eval prompt format (Lin, 2024) in a\nzero-shot setting."}]}
{"conversations": [{"role": "user", "content": "What are the characteristics of RAG as described in the paper?"}, {"role": "assistant", "content": "Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an\naverage of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0."}]}
{"conversations": [{"role": "user", "content": "What are the limitations of DeepSeek?"}, {"role": "assistant", "content": "In the future, we plan to invest in research across the following directions for DeepSeek-R1. \u2022 General Capability:Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3\nin tasks such as function calling, multi-turn, complex role-playing, and JSON output. \u2022 Language Mixing:DeepSeek-R1 is currently optimized for Chinese and English, which\nmay result in language mixing issues when handling queries in other languages."}]}
{"conversations": [{"role": "user", "content": "What is the relationship between Distill and OpenAI?"}, {"role": "assistant", "content": "pass@1 cons@64 pass@1 pass@1 pass@1 rating\nGPT-4o-0513 9.3 13.4 74.6 49.9 32.9 759\nClaude-3.5-Sonnet-1022 16.0 26.7 78.3 65.0 38.9 717\nOpenAI-o1-mini 63.6 80.0 90.0 60.0 53.8 1820\nQwQ-32B-Preview 50.0 60.0 90.6 54.5 41.9 1316\nDeepSeek-R1-Distill-Qwen-1.5B28.9 52.7 83.9 33.8 16.9 954\nDeepSeek-R1-Distill-Qwen-7B 55.5 83.3 92.8 49.1 37.6 1189\nDeepSeek-R1-Distill-Qwen-14B69.7 80.0 93.9 59.1 53.1 1481\nDeepSeek-R1-Distill-Qwen-32B 72.6 83.3 94.3 62.1 57.2 1691\nDeepSeek-R1-Distill-Llama-8B 50.4 80.0 89.1 49.0 39.6 1205\nDeepSeek-R1-Distill-Llama-70B70.0 86.7 94.5 65.2 57.5 1633\nTable 5 |Comparison of DeepSeek-R1 distilled models and other comparable models on\nreasoning-related benchmarks. As shown in Table 5, simply distilling DeepSeek-R1\u2019s outputs enables the efficient DeepSeek-\nR1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non-\nreasoning models like GPT-4o-0513 across the board."}]}
{"conversations": [{"role": "user", "content": "Compare User and Assistant."}, {"role": "assistant", "content": "A conversation between User and Assistant. User: prompt."}]}
{"conversations": [{"role": "user", "content": "What are the key components of Lanctot?"}, {"role": "assistant", "content": "Lanctot, L."}]}
{"conversations": [{"role": "user", "content": "What are the advantages of Contributions?"}, {"role": "assistant", "content": "Contributions and Acknowledgments\nCore Contributors\nDaya Guo\nDejian Yang\nHaowei Zhang\nJunxiao Song\nRuoyu Zhang\nRunxin Xu\nQihao Zhu\nShirong Ma\nPeiyi Wang\nXiao Bi\nXiaokang Zhang\nXingkai Yu\nYu Wu\nZ.F."}]}
{"conversations": [{"role": "user", "content": "What is the relationship between evaluation metrics and GPT in this context?"}, {"role": "assistant", "content": "R1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non-\nreasoning models like GPT-4o-0513 across the board. DeepSeek-R1-14B surpasses QwQ-32B-\nPreview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B significantly\nexceed o1-mini on most benchmarks."}]}
{"conversations": [{"role": "user", "content": "What are the limitations of Specifically?"}, {"role": "assistant", "content": "Specifically, for each question \ud835\udc5e, GRPO samples a group of outputs {\ud835\udc5c1, \ud835\udc5c2, \u00b7\u00b7\u00b7 , \ud835\udc5c\ud835\udc3a}from the old\npolicy \ud835\udf0b\ud835\udf03\ud835\udc5c\ud835\udc59\ud835\udc51 and then optimizes the policy model \ud835\udf0b\ud835\udf03 by maximizing the following objective:\nJ\ud835\udc3a\ud835\udc45\ud835\udc43\ud835\udc42 (\ud835\udf03)= E[\ud835\udc5e\u223c\ud835\udc43(\ud835\udc44), {\ud835\udc5c\ud835\udc56}\ud835\udc3a\n\ud835\udc56=1 \u223c\ud835\udf0b\ud835\udf03\ud835\udc5c\ud835\udc59\ud835\udc51 (\ud835\udc42|\ud835\udc5e)]\n1\n\ud835\udc3a"}]}
{"conversations": [{"role": "user", "content": "What is the purpose of DeepSeek?"}, {"role": "assistant", "content": "This\nspontaneous development significantly enhances DeepSeek-R1-Zero\u2019s reasoning capabilities,\nenabling it to tackle more challenging tasks with greater efficiency and accuracy. Aha Moment of DeepSeek-R1-ZeroA particularly intriguing phenomenon observed during\nthe training of DeepSeek-R1-Zero is the occurrence of an \u201caha moment\u201d. During this phase,\nDeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial\napproach."}]}
{"conversations": [{"role": "user", "content": "What is the purpose of language model in the context of this research?"}, {"role": "assistant", "content": "ZeroEval: A Unified Framework for Evaluating Language Models, July 2024."}]}
{"conversations": [{"role": "user", "content": "How can Wang be implemented?"}, {"role": "assistant", "content": "Process Reward Model (PRM)PRM is a reasonable method to guide the model toward better\napproaches for solving reasoning tasks (Lightman et al., 2023; Uesato et al., 2022; Wang et al.,\n2023)."}]}
{"conversations": [{"role": "user", "content": "What are the characteristics of reinforcement learning as described in the paper?"}, {"role": "assistant", "content": "5\n2.2 DeepSeek-R1-Zero: Reinforcement Learning on the Base Model . 5\n2.2.1 Reinforcement Learning Algorithm . 6\n2.2.4 Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero 6\n2.3 DeepSeek-R1: Reinforcement Learning with Cold Start ."}]}
{"conversations": [{"role": "user", "content": "What metrics were used to evaluate RAG in the paper?"}, {"role": "assistant", "content": "DeepSeek-R1 is more powerful,\nleveraging cold-start data alongside iterative RL fine-tuning."}]}
{"conversations": [{"role": "user", "content": "How does LLM compare to GPT according to the research?"}, {"role": "assistant", "content": "Hello GPT-4o, 2024a. URL https://openai.com/index/hello-gpt-4o/. Learning to reason with llms, 2024b. URL https://openai.com/index/learnin\ng-to-reason-with-llms/ ."}]}
{"conversations": [{"role": "user", "content": "What is reinforcement learning according to the paper?"}, {"role": "assistant", "content": "Deepseek-prover-v1.5: Harnessing\nproof assistant feedback for reinforcement learning and monte-carlo tree search, 2024."}]}
{"conversations": [{"role": "user", "content": "What are the key components of fine-tuning mentioned in the research?"}, {"role": "assistant", "content": "DeepSeek-R1-Zero to attain robust reasoning capabilities without the need for any supervised\nfine-tuning data."}]}
{"conversations": [{"role": "user", "content": "Explain the concept of Mayer."}, {"role": "assistant", "content": "Mayer, P ."}]}
{"conversations": [{"role": "user", "content": "What is RAG according to the paper?"}, {"role": "assistant", "content": "Moving forward, we plan to explore how long CoT can be leveraged to enhance tasks in\nthese fields."}]}
{"conversations": [{"role": "user", "content": "How was language model implemented according to the paper?"}, {"role": "assistant", "content": "Training language models to self-correct via reinforcement learning."}]}
{"conversations": [{"role": "user", "content": "What are the key components of Evaluation Prompts Following?"}, {"role": "assistant", "content": "Evaluation Prompts Following the setup in DeepSeek-V3, standard benchmarks such as\nMMLU, DROP , GPQA Diamond, and SimpleQA are evaluated using prompts from the simple-\nevals framework."}]}
{"conversations": [{"role": "user", "content": "How does the paper define fine-tuning?"}, {"role": "assistant", "content": "By initiating RL directly from the base model, we can closely monitor the model\u2019s\nprogression without the influence of the supervised fine-tuning stage."}]}
{"conversations": [{"role": "user", "content": "How can Sonnet be implemented?"}, {"role": "assistant", "content": "The results are promising: DeepSeek-R1-Distill-Qwen-1.5B outperforms GPT-4o\nand Claude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH."}]}
{"conversations": [{"role": "user", "content": "Explain the concept of Bench Verified."}, {"role": "assistant", "content": "Experiment\nBenchmarks We evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema\net al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al.,\n2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al.,\n2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI,\n11"}]}
{"conversations": [{"role": "user", "content": "What is the purpose of Zero?"}, {"role": "assistant", "content": "R1-Zero can be further augmented through the application of majority voting. For example,\nwhen majority voting is employed on the AIME benchmark, DeepSeek-R1-Zero\u2019s performance\nescalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-o1-0912. The\nability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without\nmajority voting, highlights its strong foundational capabilities and its potential for further\nadvancements in reasoning tasks."}]}
{"conversations": [{"role": "user", "content": "What is the purpose of Ren\nZehui Ren\nZhangli Sha\nZhe Fu\nZhean Xu\nZhenda Xie\nZhengyan Zhang\nZhewen Hao\nZhicheng Ma\nZhigang Yan\nZhiyu Wu\nZihui Gu?"}, {"role": "assistant", "content": "Ren\nZehui Ren\nZhangli Sha\nZhe Fu\nZhean Xu\nZhenda Xie\nZhengyan Zhang\nZhewen Hao\nZhicheng Ma\nZhigang Yan\nZhiyu Wu\nZihui Gu\n21"}]}
{"conversations": [{"role": "user", "content": "How does the paper define GPT?"}, {"role": "assistant", "content": "DeepSeek-R1 Evaluation\nBenchmark (Metric)\nClaude-3.5- GPT-4o DeepSeek OpenAI OpenAI DeepSeek\nSonnet-1022 0513 V3 o1-mini o1-1217 R1\nArchitecture - - MoE - - MoE\n# Activated Params - - 37B - - 37B\n# Total Params - - 671B - - 671B\nEnglish\nMMLU (Pass@1) 88.3 87.2 88.5 85.2 91.8 90.8\nMMLU-Redux (EM) 88.9 88.0 89.1 86.7 - 92.9\nMMLU-Pro (EM) 78.0 72.6 75.9 80.3 - 84.0\nDROP (3-shot F1) 88.3 83.7 91.6 83.9 90.2 92.2\nIF-Eval (Prompt Strict) 86.5 84.3 86.1 84.8 - 83.3\nGPQA Diamond (Pass@1) 65.0 49.9 59.1 60.0 75.7 71.5\nSimpleQA (Correct) 28.4 38.2 24.9 7.0 47.0 30.1\nFRAMES (Acc.) 72.5 80.5 73.3 76.9 - 82.5\nAlpacaEval2.0 (LC-winrate) 52.0 51.1 70.0 57.8 - 87.6\nArenaHard (GPT-4-1106) 85.2 80.4 85.5 92.0 - 92.3\nCode\nLiveCodeBench (Pass@1-COT) 38.9 32.9 36.2 53.8 63.4 65.9\nCodeforces (Percentile) 20.3 23.6 58.7 93.4 96.6 96.3\nCodeforces (Rating) 717 759 1134 1820 2061 2029\nSWE Verified (Resolved) 50.8 38.8 42.0 41.6 48.9 49.2\nAider-Polyglot (Acc.) 45.3 16.0 49.6 32.9 61.7 53.3\nMath"}]}
{"conversations": [{"role": "user", "content": "What are the limitations of DeepSeek?"}, {"role": "assistant", "content": "DeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a\nmodel\u2019s ability to follow format instructions. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard,\nindicating DeepSeek-R1\u2019s strengths in writing tasks and open-domain question answering. Its\nsignificant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale\nRL, which not only boosts reasoning capabilities but also improves performance across diverse\ndomains."}]}
{"conversations": [{"role": "user", "content": "How can OpenAI be implemented?"}, {"role": "assistant", "content": "Baselines We conduct comprehensive evaluations against several strong baselines, including\nDeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217. Since accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its perfor-\nmance based on official reports."}]}
{"conversations": [{"role": "user", "content": "What is GPT according to the paper?"}, {"role": "assistant", "content": "DeepSeek-R1 avoids introducing length bias during GPT-based evaluations, further solidifying\nits robustness across multiple tasks. Distilled Model Evaluation\nModel AIME 2024 MATH-500 GPQA LiveCode CodeForcesDiamond Bench\npass@1 cons@64 pass@1 pass@1 pass@1 rating\nGPT-4o-0513 9.3 13.4 74.6 49.9 32.9 759\nClaude-3.5-Sonnet-1022 16.0 26.7 78.3 65.0 38.9 717\nOpenAI-o1-mini 63.6 80.0 90.0 60.0 53.8 1820"}]}
{"conversations": [{"role": "user", "content": "What metrics were used to evaluate benchmarks in the paper?"}, {"role": "assistant", "content": "Additionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring\nlong-context understanding, substantially outperforming DeepSeek-V3 on long-context\nbenchmarks."}]}
{"conversations": [{"role": "user", "content": "What are the characteristics of GPT as described in the paper?"}, {"role": "assistant", "content": "Baselines We conduct comprehensive evaluations against several strong baselines, including\nDeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217."}]}
{"conversations": [{"role": "user", "content": "What is reinforcement learning according to the paper?"}, {"role": "assistant", "content": "Second, while distillation strategies are both economical and effective, advancing\nbeyond the boundaries of intelligence may still require more powerful base models and larger-\nscale reinforcement learning."}]}
{"conversations": [{"role": "user", "content": "What is the purpose of Monte Carlo Tree Search?"}, {"role": "assistant", "content": "Several prior\nworks have explored various approaches, including process-based reward models (Lightman\net al., 2023; Uesato et al., 2022; Wang et al., 2023), reinforcement learning (Kumar et al., 2024),\nand search algorithms such as Monte Carlo Tree Search and Beam Search (Feng et al., 2024; Trinh\net al., 2024; Xin et al., 2024)."}]}
{"conversations": [{"role": "user", "content": "Explain the concept of Qwen."}, {"role": "assistant", "content": "Distillation: Empower Small Models with Reasoning Capability\nTo equip more efficient smaller models with reasoning capabilities like DeepSeek-R1, we directly\nfine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using\nthe 800k samples curated with DeepSeek-R1, as detailed in \u00a72.3.3."}]}
{"conversations": [{"role": "user", "content": "What is the purpose of DeepSeek?"}, {"role": "assistant", "content": "This approach allows the model to explore chain-of-thought (CoT) for\nsolving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-\nR1-Zero demonstrates capabilities such as self-verification, reflection, and generating\nlong CoTs, marking a significant milestone for the research community. \u2022 We introduce our pipeline to develop DeepSeek-R1."}]}
{"conversations": [{"role": "user", "content": "Compare Song and Zhang."}, {"role": "assistant", "content": "Song, Z."}]}
{"conversations": [{"role": "user", "content": "What is the purpose of Kaichao You\nKaige Gao\nKang Guan\nKexin Huang\nKuai Yu\nLean Wang\nLecong Zhang\nLiang Zhao\nLitong Wang\nLiyue Zhang\nLei Xu\nLeyi Xia\nMingchuan Zhang\nMinghua Zhang\nMinghui Tang\nMingxu Zhou\nMeng Li\nMiaojun Wang\nMingming Li\nNing Tian\nPanpan Huang\nPeng Zhang\nQiancheng Wang\nQinyu Chen\nQiushi Du\nRuiqi Ge?"}, {"role": "assistant", "content": "Cai\nJiaqi Ni\nJian Liang\nJin Chen\nKai Dong\nKai Hu*\nKaichao You\nKaige Gao\nKang Guan\nKexin Huang\nKuai Yu\nLean Wang\nLecong Zhang\nLiang Zhao\nLitong Wang\nLiyue Zhang\nLei Xu\nLeyi Xia\nMingchuan Zhang\nMinghua Zhang\nMinghui Tang\nMingxu Zhou\nMeng Li\nMiaojun Wang\nMingming Li\nNing Tian\nPanpan Huang\nPeng Zhang\nQiancheng Wang\nQinyu Chen\nQiushi Du\nRuiqi Ge*\nRuisong Zhang\nRuizhe Pan"}]}
{"conversations": [{"role": "user", "content": "What is the purpose of distillation in the context of this research?"}, {"role": "assistant", "content": "Distillation v.s. However, there is still one question left: can the model achieve comparable performance\nthrough the large-scale RL training discussed in the paper without distillation?"}]}
{"conversations": [{"role": "user", "content": "How can AlphaZero be implemented?"}, {"role": "assistant", "content": "Monte Carlo Tree Search (MCTS)Inspired by AlphaGo (Silver et al., 2017b) and AlphaZero (Sil-\nver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time\ncompute scalability."}]}
{"conversations": [{"role": "user", "content": "What are the characteristics of language model as described in the paper?"}, {"role": "assistant", "content": "Introduction\nIn recent years, Large Language Models (LLMs) have been undergoing rapid iteration and\nevolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap\ntowards Artificial General Intelligence (AGI)."}]}
{"conversations": [{"role": "user", "content": "What are the characteristics of LLM as described in the paper?"}, {"role": "assistant", "content": "In this section, we\nexplore the potential of LLMs to develop reasoning capabilities without any supervised data,\nfocusing on their self-evolution through a pure reinforcement learning process."}]}
{"conversations": [{"role": "user", "content": "What are the key components of reinforcement learning mentioned in the research?"}, {"role": "assistant", "content": "et al., 2024), its advantages are limited compared to the additional computational overhead it\nintroduces during the large-scale reinforcement learning process in our experiments."}]}
{"conversations": [{"role": "user", "content": "What are the advantages of GPQA Diamond?"}, {"role": "assistant", "content": "\u2022 Knowledge: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-\nR1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores\nof 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond."}]}
{"conversations": [{"role": "user", "content": "How does Shao work?"}, {"role": "assistant", "content": "Specifically, we use DeepSeek-V3-Base as the base model and employ\nGRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning."}]}
{"conversations": [{"role": "user", "content": "How does DeepSeek work?"}, {"role": "assistant", "content": "(2)\nOn coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks,\nas it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in\nthe competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than\nDeepSeek-V3, which could help developers in real world tasks. \u2022 Knowledge: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-\nR1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores\nof 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond."}]}
{"conversations": [{"role": "user", "content": "How does Process work?"}, {"role": "assistant", "content": "6\n2.2.4 Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero 6\n2.3 DeepSeek-R1: Reinforcement Learning with Cold Start ."}]}
{"conversations": [{"role": "user", "content": "What are the characteristics of reinforcement learning as described in the paper?"}, {"role": "assistant", "content": "This is also an aha moment for us,\nallowing us to witness the power and beauty of reinforcement learning."}]}
{"conversations": [{"role": "user", "content": "What is the purpose of Zero?"}, {"role": "assistant", "content": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\nReinforcement Learning\nDeepSeek-AI\nresearch@deepseek.com\nAbstract\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without super-\nvised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing\nreasoning behaviors."}]}
{"conversations": [{"role": "user", "content": "What are the key components of GPT mentioned in the research?"}, {"role": "assistant", "content": "A similar trend is observed where\nOpenAI-o1 surpasses GPT-4o on this benchmark."}]}
{"conversations": [{"role": "user", "content": "What is the relationship between DeepSeek and AIME?"}, {"role": "assistant", "content": "DeepSeek-\nR1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Addi-\ntionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500,\nand 57.2% on LiveCodeBench. Summary of Evaluation Results\n\u2022 Reasoning tasks: (1) DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly\nsurpassing OpenAI-o1-1217."}]}
{"conversations": [{"role": "user", "content": "What are the key components of Hubert?"}, {"role": "assistant", "content": "Hubert, L."}]}
{"conversations": [{"role": "user", "content": "Compare DeepSeek and Readability."}, {"role": "assistant", "content": "include:\n\u2022 Readability: A key limitation of DeepSeek-R1-Zero is that its content is often not suitable\nfor reading. In contrast, when creating cold-start data for DeepSeek-R1,\nwe design a readable pattern that includes a summary at the end of each response and\nfilters out responses that are not reader-friendly. \u2022 Potential: By carefully designing the pattern for cold-start data with human priors, we\nobserve better performance against DeepSeek-R1-Zero."}]}
{"conversations": [{"role": "user", "content": "How can LeetCode be implemented?"}, {"role": "assistant", "content": "Similarly, for LeetCode problems, a compiler can be\nused to generate feedback based on predefined test cases."}]}
{"conversations": [{"role": "user", "content": "What is the purpose of language model in the context of this research?"}, {"role": "assistant", "content": "Evaluating large language models trained on code. Alphazero-like\ntree-search can guide large language model decoding and training, 2024."}]}
{"conversations": [{"role": "user", "content": "What are the key components of benchmarks mentioned in the research?"}, {"role": "assistant", "content": "AIDER-related\nbenchmarks are measured using a \"diff\" format."}]}
{"conversations": [{"role": "user", "content": "How does the paper define benchmarks?"}, {"role": "assistant", "content": "DeepSeek-R1-14B surpasses QwQ-32B-\nPreview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B significantly\nexceed o1-mini on most benchmarks."}]}
{"conversations": [{"role": "user", "content": "What is the purpose of DeepSeek?"}, {"role": "assistant", "content": "Training Template\nTo train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides\nthe base model to adhere to our specified instructions. As depicted in Table 1, this template\nrequires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero\nPerformance of DeepSeek-R1-Zero Figure 2 depicts the performance trajectory of DeepSeek-\nR1-Zero on the AIME 2024 benchmark throughout the RL training process."}]}
{"conversations": [{"role": "user", "content": "Compare Accuracy and Zero."}, {"role": "assistant", "content": "types of rewards:\n\u2022 Accuracy rewards: The accuracy reward model evaluates whether the response is correct."}]}
{"conversations": [{"role": "user", "content": "What is fine-tuning according to the paper?"}, {"role": "assistant", "content": "In this study, we demonstrate that reasoning capabilities can be significantly\nimproved through large-scale reinforcement learning (RL), even without using supervised\nfine-tuning (SFT) as a cold start."}]}
{"conversations": [{"role": "user", "content": "What is Furthermore?"}, {"role": "assistant", "content": "Furthermore, performance can be further enhanced with"}]}
{"conversations": [{"role": "user", "content": "What is distillation according to the paper?"}, {"role": "assistant", "content": "We further explore distillation from DeepSeek-R1 to smaller dense models."}]}
{"conversations": [{"role": "user", "content": "What is the relationship between Supervised Fine and Unlike?"}, {"role": "assistant", "content": "Rejection Sampling and Supervised Fine-Tuning\nWhen reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT\n(Supervised Fine-Tuning) data for the subsequent round."}]}
{"conversations": [{"role": "user", "content": "How can Second be implemented?"}, {"role": "assistant", "content": "Second, the value model\ndirectly influences the quality of generation since it guides each step of the search process."}]}
{"conversations": [{"role": "user", "content": "What are the characteristics of distillation as described in the paper?"}, {"role": "assistant", "content": "Our findings indicate that\nthis straightforward distillation method significantly enhances the reasoning abilities of smaller\nmodels. Our primary goal here is to\ndemonstrate the effectiveness of the distillation technique, leaving the exploration of the RL\nstage to the broader research community."}]}
{"conversations": [{"role": "user", "content": "What are the characteristics of LLM as described in the paper?"}, {"role": "assistant", "content": "Our goal is to explore the potential of LLMs to develop\nreasoning capabilities without any supervised data, focusing on their self-evolution through\na pure RL process."}]}
{"conversations": [{"role": "user", "content": "What is the purpose of DeepSeek?"}, {"role": "assistant", "content": "In the following sections, we present: (1)\nDeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and\n(2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of\nlong Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to\nsmall dense models. DeepSeek-R1-Zero: Reinforcement Learning on the Base Model\nReinforcement learning has demonstrated significant effectiveness in reasoning tasks, as ev-\nidenced by our previous works (Shao et al., 2024; Wang et al., 2023)."}]}
{"conversations": [{"role": "user", "content": "What is Redux?"}, {"role": "assistant", "content": "DeepSeek-R1 Evaluation\nBenchmark (Metric)\nClaude-3.5- GPT-4o DeepSeek OpenAI OpenAI DeepSeek\nSonnet-1022 0513 V3 o1-mini o1-1217 R1\nArchitecture - - MoE - - MoE\n# Activated Params - - 37B - - 37B\n# Total Params - - 671B - - 671B\nEnglish\nMMLU (Pass@1) 88.3 87.2 88.5 85.2 91.8 90.8\nMMLU-Redux (EM) 88.9 88.0 89.1 86.7 - 92.9\nMMLU-Pro (EM) 78.0 72.6 75.9 80.3 - 84.0\nDROP (3-shot F1) 88.3 83.7 91.6 83.9 90.2 92.2\nIF-Eval (Prompt Strict) 86.5 84.3 86.1 84.8 - 83.3\nGPQA Diamond (Pass@1) 65.0 49.9 59.1 60.0 75.7 71.5\nSimpleQA (Correct) 28.4 38.2 24.9 7.0 47.0 30.1\nFRAMES (Acc.) 72.5 80.5 73.3 76.9 - 82.5\nAlpacaEval2.0 (LC-winrate) 52.0 51.1 70.0 57.8 - 87.6\nArenaHard (GPT-4-1106) 85.2 80.4 85.5 92.0 - 92.3\nCode\nLiveCodeBench (Pass@1-COT) 38.9 32.9 36.2 53.8 63.4 65.9\nCodeforces (Percentile) 20.3 23.6 58.7 93.4 96.6 96.3\nCodeforces (Rating) 717 759 1134 1820 2061 2029\nSWE Verified (Resolved) 50.8 38.8 42.0 41.6 48.9 49.2\nAider-Polyglot (Acc.) 45.3 16.0 49.6 32.9 61.7 53.3\nMath"}]}
{"conversations": [{"role": "user", "content": "What is the purpose of Zero?"}, {"role": "assistant", "content": "Cold Start\nUnlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from\nthe base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data\nto fine-tune the model as the initial RL actor. To collect such data, we have explored several\napproaches: using few-shot prompting with a long CoT as an example, directly prompting\nmodels to generate detailed answers with reflection and verification, gathering DeepSeek-R1-\nZero outputs in a readable format, and refining the results through post-processing by human\nannotators."}]}
{"conversations": [{"role": "user", "content": "What are the key components of distillation mentioned in the research?"}, {"role": "assistant", "content": "We further explore distillation from DeepSeek-R1 to smaller dense models. Using Qwen2.5-\n32B (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying\nRL on it."}]}
{"conversations": [{"role": "user", "content": "Compare Aider and DeepSeek."}, {"role": "assistant", "content": "Codeforces (Rating) 717 759 1134 1820 2061 2029\nSWE Verified (Resolved) 50.8 38.8 42.0 41.6 48.9 49.2\nAider-Polyglot (Acc.) 45.3 16.0 49.6 32.9 61.7 53.3\nMath\nAIME 2024 (Pass@1) 16.0 9.3 39.2 63.6 79.2 79.8\nMATH-500 (Pass@1) 78.3 74.6 90.2 90.0 96.4 97.3\nCNMO 2024 (Pass@1) 13.1 10.8 43.2 67.6 - 78.8\nChinese\nCLUEWSC (EM) 85.4 87.9 90.9 89.9 - 92.8\nC-Eval (EM) 76.7 76.0 86.5 68.9 - 91.8\nC-SimpleQA (Correct) 55.4 58.7 68.0 40.3 - 63.7\nTable 4 |Comparison between DeepSeek-R1 and other representative models."}]}
{"conversations": [{"role": "user", "content": "What are the characteristics of distillation as described in the paper?"}, {"role": "assistant", "content": "Distillation: Smaller Models Can Be Powerful Too\n\u2022 We demonstrate that the reasoning patterns of larger models can be distilled into smaller\nmodels, resulting in better performance compared to the reasoning patterns discovered\nthrough RL on small models."}]}
{"conversations": [{"role": "user", "content": "Explain the concept of Zero."}, {"role": "assistant", "content": "R1-Zero can be further augmented through the application of majority voting. For example,\nwhen majority voting is employed on the AIME benchmark, DeepSeek-R1-Zero\u2019s performance\nescalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-o1-0912. The\nability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without\nmajority voting, highlights its strong foundational capabilities and its potential for further\nadvancements in reasoning tasks."}]}
{"conversations": [{"role": "user", "content": "What is distillation according to the paper?"}, {"role": "assistant", "content": "11\n2.4 Distillation: Empower Small Models with Reasoning Capability . 14\n4 Discussion 14\n4.1 Distillation v.s."}]}
{"conversations": [{"role": "user", "content": "What is the purpose of benchmarks in the context of this research?"}, {"role": "assistant", "content": "and Claude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH."}]}
